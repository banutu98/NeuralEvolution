{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NR_EPOCHS = 20\n",
    "POP_SIZE = 30\n",
    "HIGHER_BOUND = 10\n",
    "LOWER_BOUND = -10\n",
    "# The final result is 18 bits. Couldn't we use 32 bit unsigned\n",
    "# integers instead? It could be faster, but not sure...\n",
    "INTERVALS_NR = (HIGHER_BOUND - LOWER_BOUND) * 10 ** 4\n",
    "BITS_NR = math.ceil(np.log2(INTERVALS_NR))\n",
    "MUTATION_PROB = 0.3\n",
    "CROSSOVER_PROB = 0.6\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert(y):\n",
    "    decimal = int(y, 2)\n",
    "    rez_final = LOWER_BOUND + decimal * (HIGHER_BOUND - LOWER_BOUND) / ((2 ** BITS_NR) - 1)\n",
    "    return rez_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return np.divide(1, (1 + np.exp(-z)))\n",
    "\n",
    "\n",
    "def soft_max(z):\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / e_z.sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_network(data, computed_weights, computed_biases):\n",
    "    first_layer_weights, second_layer_weights = computed_weights[0], computed_weights[1]\n",
    "    first_layer_biases, second_layer_biases = computed_biases[0], computed_biases[1]\n",
    "    third_layer_biases, third_layer_weights = computed_biases[2], computed_weights[2]\n",
    "    good_predictions = 0\n",
    "    for x, label in zip(data[0], data[1]):\n",
    "        z1 = np.dot(x, first_layer_weights) + first_layer_biases\n",
    "        z1 = sigmoid(z1)\n",
    "        z2 = np.dot(z1, second_layer_weights) + second_layer_biases\n",
    "        z2 = sigmoid(z2)\n",
    "        z3 = np.dot(z2, third_layer_weights) + third_layer_biases\n",
    "        y3 = sigmoid(z3)\n",
    "        prediction = np.argmax(y3)\n",
    "        if prediction == label:\n",
    "            good_predictions += 1\n",
    "    return good_predictions / len(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fitness_network(x, y, converted_parameters):\n",
    "    first_layer_weights = converted_parameters[0]\n",
    "    second_layer_weights = converted_parameters[1]\n",
    "    third_layer_weights = converted_parameters[2]\n",
    "    first_layer_biases = converted_parameters[3]\n",
    "    second_layer_biases = converted_parameters[4]\n",
    "    third_layer_biases = converted_parameters[5]\n",
    "    y_pred = list()\n",
    "    for start_idx in range(0, x.shape[0], BATCH_SIZE):\n",
    "        x_batch = x[start_idx:start_idx + BATCH_SIZE]\n",
    "        # No need to add a dimension to x, implicit broadcasting\n",
    "        # rules are faster.\n",
    "        z1 = np.matmul(x_batch, first_layer_weights) + first_layer_biases\n",
    "        # expit may be better, although it's debatable.\n",
    "        z1 = expit(z1)\n",
    "        z2 = np.matmul(z1, second_layer_weights) + second_layer_biases\n",
    "        z2 = expit(z2)\n",
    "        z3 = np.matmul(z2, third_layer_weights) + third_layer_biases\n",
    "        y3 = expit(z3)\n",
    "        y_pred.append(y3)\n",
    "    # axis=1, 0 is population size.\n",
    "    # Now it works even if the number of instances is not divisible\n",
    "    # by the size of the minibatch.\n",
    "    y_pred = np.concatenate(y_pred, axis=1)\n",
    "    y_true = np.broadcast_to(y, (y_pred.shape[0], *y.shape))\n",
    "    return [1 / log_loss(y_true[i], y_pred[i])\n",
    "           for i in range(y_pred.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_individual_for_network(individual):\n",
    "    \"\"\"\n",
    "    This function is called for the same individuals on every call\n",
    "    of fitness_network. There is no need for this.\n",
    "    \"\"\"\n",
    "    convert_vector = np.vectorize(convert)\n",
    "    first_layer_weights = np.reshape(np.apply_along_axis(convert_vector, 0, individual[:78400]), (784, 100))\n",
    "    second_layer_weights = np.reshape(np.apply_along_axis(convert_vector, 0, individual[78400:79400]), (100, 10))\n",
    "    third_layer_weights = np.reshape(np.apply_along_axis(convert_vector, 0, individual[79400:79500]), (10, 10))\n",
    "    first_layer_biases = np.apply_along_axis(convert_vector, 0, individual[79500:79600])\n",
    "    second_layer_biases = np.apply_along_axis(convert_vector, 0, individual[79600:79610])\n",
    "    third_layer_biases = np.apply_along_axis(convert_vector, 0, individual[79610:])\n",
    "    return ([first_layer_weights, second_layer_weights, third_layer_weights],\n",
    "            [first_layer_biases, second_layer_biases, third_layer_biases])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def selection(population, train_set):\n",
    "    new_population = list()\n",
    "    individual_fitness = [fitness_network(train_set, *prepare_individual_for_network(ind)) for ind in population]\n",
    "    total_fitness = sum(individual_fitness)\n",
    "    individual_probabilities = [ind_fitness / total_fitness for ind_fitness in individual_fitness]\n",
    "    cumulative_probabilities = [0]\n",
    "    for i in range(POP_SIZE):\n",
    "        cumulative_probabilities.append(cumulative_probabilities[i] + individual_probabilities[i])\n",
    "    size = 0\n",
    "    while size < POP_SIZE:\n",
    "        r = random.uniform(0.0001, 1)\n",
    "        for i in range(POP_SIZE):\n",
    "            if cumulative_probabilities[i] < r <= cumulative_probabilities[i + 1]:\n",
    "                if size == POP_SIZE:\n",
    "                    break\n",
    "                new_population.append(population[i])\n",
    "                size += 1\n",
    "    return new_population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def upgrade(population):\n",
    "    for individual in population:\n",
    "        for b in range(len(individual)):\n",
    "            new_bit_array = ''\n",
    "            for bit in individual[b]:\n",
    "                if random.uniform(0, 1) < MUTATION_PROB:\n",
    "                    if bit == '1':\n",
    "                        new_bit_array += '0'\n",
    "                    else:\n",
    "                        new_bit_array += '1'\n",
    "                else:\n",
    "                    new_bit_array += bit\n",
    "            individual[b] = new_bit_array\n",
    "\n",
    "    cross_over_indexes = [i for i in range(POP_SIZE) if random.uniform(0, 1) < 0.6]\n",
    "    if len(cross_over_indexes) % 2 != 0:\n",
    "        cross_over_indexes = cross_over_indexes[:-1]\n",
    "    for i in range(0, len(cross_over_indexes), 2):\n",
    "        first_child = list()\n",
    "        second_child = list()\n",
    "        for k in range(len(population[cross_over_indexes[i]])):\n",
    "            bit_array_length = len(population[cross_over_indexes[i]][k])\n",
    "            r = random.randint(0, bit_array_length - 1)\n",
    "            new_first_array = population[cross_over_indexes[i]][k][:r] + population[cross_over_indexes[i + 1]][k][r:]\n",
    "            new_second_array = population[cross_over_indexes[i]][k][r:] + population[cross_over_indexes[i + 1]][k][:r]\n",
    "            first_child.append(new_first_array)\n",
    "            second_child.append(new_second_array)\n",
    "        population[cross_over_indexes[i]] = list(first_child)\n",
    "        population[cross_over_indexes[i + 1]] = list(second_child)\n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_population(x, y, population):\n",
    "    convert_vector = np.vectorize(convert)\n",
    "    # best_individual = np.zeros(len(population[0]))\n",
    "    best_individual = None\n",
    "    fitness_values = fitness_network(x, y,\n",
    "                                     tuple(convert_vector(layer)\n",
    "                                           for params in population\n",
    "                                           for layer in params))\n",
    "    local_best = np.argmax(fitness_values)\n",
    "    best = fitness_values[local_best]\n",
    "    # Ugly, but faster than other ways I can think of.\n",
    "    best_individual = (# weights\n",
    "                       population[0][0][local_best],\n",
    "                       population[0][1][local_best],\n",
    "                       population[0][2][local_best],\n",
    "                       # biases\n",
    "                       population[1][0][local_best],\n",
    "                       population[1][1][local_best],\n",
    "                       population[1][2][local_best])\n",
    "    return best, best_individual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_population():\n",
    "    first_layer_weights = np.apply_along_axis(''.join,\n",
    "                                              1,\n",
    "                                              np.random.choice(['0', '1'], size=(POP_SIZE, BITS_NR, 784, 100)))\n",
    "    second_layer_weights = np.apply_along_axis(''.join,\n",
    "                                               1,\n",
    "                                               np.random.choice(['0', '1'], size=(POP_SIZE, BITS_NR, 100, 10)))\n",
    "    third_layer_weights = np.apply_along_axis(''.join,\n",
    "                                              1,\n",
    "                                              np.random.choice(['0', '1'], size=(POP_SIZE, BITS_NR, 10, 10)))\n",
    "    \n",
    "    first_layer_biases = np.apply_along_axis(''.join,\n",
    "                                             2,\n",
    "                                             np.random.choice(['0', '1'], size=(POP_SIZE, 100, BITS_NR)))[:, np.newaxis, :]\n",
    "    second_layer_biases = np.apply_along_axis(''.join,\n",
    "                                              2,\n",
    "                                              np.random.choice(['0', '1'], size=(POP_SIZE, 10, BITS_NR)))[:, np.newaxis, :]\n",
    "    third_layer_biases = np.apply_along_axis(''.join,\n",
    "                                             2,\n",
    "                                             np.random.choice(['0', '1'], size=(POP_SIZE, 10, BITS_NR))) [:, np.newaxis, :]\n",
    "    weights = (first_layer_weights, second_layer_weights, third_layer_weights)\n",
    "    biases = (first_layer_biases, second_layer_biases, third_layer_biases)\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_all_individuals():\n",
    "    population = []\n",
    "    for _ in range(POP_SIZE):\n",
    "        first_layer_weights = np.apply_along_axis(''.join, 1, np.random.choice(['0', '1'], size=(78400, BITS_NR)))\n",
    "        second_layer_weights = np.apply_along_axis(''.join, 1, np.random.choice(['0', '1'], size=(1000, BITS_NR)))\n",
    "        third_layer_weights = np.apply_along_axis(''.join, 1, np.random.choice(['0', '1'], size=(100, BITS_NR)))\n",
    "\n",
    "        first_layer_biases = np.apply_along_axis(''.join, 1, np.random.choice(['0', '1'], size=(100, BITS_NR)))\n",
    "        second_layer_biases = np.apply_along_axis(''.join, 1, np.random.choice(['0', '1'], size=(10, BITS_NR)))\n",
    "        third_layer_biases = np.apply_along_axis(''.join, 1, np.random.choice(['0', '1'], size=(10, BITS_NR)))\n",
    "        individual = np.concatenate([first_layer_weights, second_layer_weights, third_layer_weights,\n",
    "                                     first_layer_biases, second_layer_biases, third_layer_biases])\n",
    "        population.append(individual)\n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        x_train, y_train = train_set\n",
    "        x_valid, y_valid = valid_set\n",
    "        x_test, y_test = test_set\n",
    "    #best = 0\n",
    "    population = generate_population()\n",
    "    best, best_individual = evaluate_population(train_set, population)\n",
    "    for i in range(NR_EPOCHS):\n",
    "        print(f'Current epoch: {i}')\n",
    "        population = selection(population, train_set)\n",
    "        population = list(upgrade(population))\n",
    "        new_best, new_best_individual = evaluate_population(x_train, y_train, population)\n",
    "        if new_best > best:\n",
    "            best = new_best\n",
    "            #best_individual = np.copy(temp_individual)\n",
    "            best_individual = new_best_individual\n",
    "    best_score = test_network(train_set, *prepare_individual_for_network(best_individual))\n",
    "    print(f'The network achieved an accuracy of {best_score * 100} percent on training set!')\n",
    "    best_score = test_network(test_set, *prepare_individual_for_network(best_individual))\n",
    "    print(f'The network achieved an accuracy of {best_score * 100} percent on testing set!')\n",
    "    print(f'Time taken: {time.time() - start_time} seconds!') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    x_train, y_train = train_set\n",
    "    x_valid, y_valid = valid_set\n",
    "    x_test, y_test = test_set\n",
    "population = generate_population()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 s ± 615 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "#%lprun -f evaluate_population evaluate_population(x_train, y_train, population)\n",
    "%timeit evaluate_population(x_train, y_train, population)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import expit, softmax\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_EPOCHS = 200\n",
    "POP_SIZE = 30\n",
    "ELITISM_NR = 10\n",
    "HIGHER_BOUND = 1\n",
    "LOWER_BOUND = -1\n",
    "# 95% of values will be between LOWER_BOUND and HIGHER_BOUND\n",
    "# if mean centered\n",
    "SCALE = ((HIGHER_BOUND - LOWER_BOUND) / 2) / 2\n",
    "INTERVALS_NR = (HIGHER_BOUND - LOWER_BOUND) * 10 ** 4\n",
    "BITS_NR = math.ceil(np.log2(INTERVALS_NR))\n",
    "MUTATION_PROB = 0.1\n",
    "CROSSOVER_PROB = 0.6\n",
    "BATCH_SIZE = 256\n",
    "# 1 input, 1 hidden, 1 output = 3 layers\n",
    "N_UNITS = [784, 16, 10]\n",
    "N_WEIGHTS = len(N_UNITS) - 1\n",
    "N_BIASES = N_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "# TODO: See which is more efficient\n",
    "def sigmoid(z):\n",
    "    return np.divide(1, (1 + np.exp(-z)))\n",
    "\n",
    "\n",
    "def expit_approx(x):\n",
    "    return 1.0 / (1 + np.abs(x))\n",
    "\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "\n",
    "# expit imported from scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_network(population, x, y):\n",
    "    losses = []\n",
    "    if not population:\n",
    "        return []\n",
    "    n_weights = len(population[0]) // 2\n",
    "    for individual in population:\n",
    "        weights = individual[:n_weights]\n",
    "        biases = individual[n_weights:]\n",
    "        y_pred = list()\n",
    "        for start_idx in range(0, x.shape[0], BATCH_SIZE):\n",
    "            x_batch = x[start_idx:start_idx + BATCH_SIZE]\n",
    "            z = x_batch\n",
    "            for i in range(n_weights - 1):\n",
    "                z = np.dot(z, weights[i]) + biases[i]\n",
    "                # expit may be better, although it's debatable.\n",
    "                z = expit(z)\n",
    "            z = np.dot(z, weights[n_weights - 1]) + biases[n_weights - 1]\n",
    "            y_final = softmax(z)\n",
    "            y_pred.append(y_final)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        losses.append(1 / np.exp(log_loss(y, y_pred)))\n",
    "    return losses\n",
    "\n",
    "\n",
    "def test_network(individual, x, y):\n",
    "    n_weights = len(individual) // 2\n",
    "    weights = individual[:n_weights]\n",
    "    biases = individual[n_weights:]\n",
    "    y_pred = list()\n",
    "    for start_idx in range(0, x.shape[0], BATCH_SIZE):\n",
    "        x_batch = x[start_idx:start_idx + BATCH_SIZE]\n",
    "        z = x_batch\n",
    "        for i in range(n_weights - 1):\n",
    "            z = np.dot(z, weights[i]) + biases[i]\n",
    "            # expit may be better, although it's debatable.\n",
    "            z = expit(z)\n",
    "        z = np.dot(z, weights[n_weights - 1] + biases[n_weights] - 1)\n",
    "        y_final = softmax(z)\n",
    "        y_pred.append(y_final)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_pred = np.apply_along_axis(np.argmax, 1, y_pred)\n",
    "    return np.sum(y_pred == y) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(pop):\n",
    "    new_pop = []\n",
    "    for indiv in pop:\n",
    "        new_indiv = []\n",
    "        for layer in indiv:\n",
    "            new_indiv.append(np.where(np.random.rand(*layer.shape) < MUTATION_PROB,\n",
    "                                      layer + np.random.normal(loc=0,\n",
    "                                                               scale=SCALE,\n",
    "                                                               size=layer.shape),\n",
    "                                      layer))\n",
    "        new_pop.append(new_indiv)\n",
    "    return new_pop\n",
    "\n",
    "\n",
    "def crossover(pop, cross_percentages):\n",
    "    def swap_weights(p, i1, i2):\n",
    "        for i1_idx, i2_idx in zip(i1, i2):\n",
    "            # choose a random layer (weights only)\n",
    "            l = random.randint(0, N_WEIGHTS - 1)\n",
    "            i = random.randint(0, p[i1_idx][l].shape[0] - 1)\n",
    "            j = random.randint(0, p[i1_idx][l].shape[1] - 1)\n",
    "            temp = p[i1_idx][l][i, j].copy()\n",
    "            p[i1_idx][l][i, j] = p[i2_idx][l][i, j]\n",
    "            p[i2_idx][l][i, j] = temp\n",
    "\n",
    "    def swap_neurons(p, i1, i2):\n",
    "        for i1_idx, i2_idx in zip(i1, i2):\n",
    "            # choose a random layer (weights and biases)\n",
    "            l = random.randint(0, N_WEIGHTS + N_BIASES - 1)\n",
    "            i = random.randint(0, p[i1_idx][l].shape[0] - 1)\n",
    "            temp = p[i1_idx][l][i].copy()\n",
    "            p[i1_idx][l][i] = p[i2_idx][l][i]\n",
    "            p[i2_idx][l][i] = temp\n",
    "\n",
    "    def swap_layers(p, i1, i2):\n",
    "        for i1_idx, i2_idx in zip(i1, i2):\n",
    "            # choose a random layer (weights and biases)\n",
    "            l = random.randint(0, N_WEIGHTS + N_BIASES - 1)\n",
    "            temp = p[i1_idx][l].copy()\n",
    "            p[i1_idx][l] = p[i2_idx][l]\n",
    "            p[i2_idx][l] = temp\n",
    "\n",
    "    def split_perc(indices, perc):\n",
    "        # Turn percentages into values between 0 and 1\n",
    "        splits = np.cumsum(perc)\n",
    "        if splits[-1] != 1:\n",
    "            raise ValueError(\"percents don't add up to 100\")\n",
    "        # Split doesn't need last percent, it will just take what is left\n",
    "        splits = splits[:-1]\n",
    "        # Turn values into indices\n",
    "        splits *= len(indices)\n",
    "        # Turn double indices into integers.\n",
    "        # CAUTION: numpy rounds to closest EVEN number when a number is halfway\n",
    "        # between two integers. So 0.5 will become 0 and 1.5 will become 2!\n",
    "        # If you want to round up in all those cases, do\n",
    "        # splits += 0.5 instead of round() before casting to int\n",
    "        splits = splits.round().astype(np.int)\n",
    "        splits = np.split(indices, splits)\n",
    "        # Make arrays of even lengths\n",
    "        for i in range(len(splits)):\n",
    "            if len(splits[i]) % 2:\n",
    "                splits[i] = np.append(splits[i],\n",
    "                                      np.random.choice(splits[i],\n",
    "                                                       size=(1,)))\n",
    "        return splits\n",
    "\n",
    "    # ACTUAL FUNCTION LOGIC STARTS HERE\n",
    "\n",
    "    cross_indices = np.arange(POP_SIZE)[np.random.rand(POP_SIZE) < CROSSOVER_PROB]\n",
    "    shuffled_indices = np.random.choice(cross_indices,\n",
    "                                        size=cross_indices.size,\n",
    "                                        replace=False)\n",
    "    weights, neurons, layers = split_perc(shuffled_indices, cross_percentages)\n",
    "    swap_weights(pop, *np.split(weights, 2))\n",
    "    swap_neurons(pop, *np.split(neurons, 2))\n",
    "    swap_layers(pop, *np.split(layers, 2))\n",
    "\n",
    "\n",
    "def upgrade(population, cross_percentages=(.3, .3, .4)):\n",
    "    new_population = mutate(population)\n",
    "    # This function modifies the matrix in-place\n",
    "    crossover(new_population, cross_percentages)\n",
    "    return new_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(population, fitness_values, elitism=False):\n",
    "    new_population = []\n",
    "    # Compute cumulative distribution.\n",
    "    total_fitness = sum(fitness_values)\n",
    "    individual_probabilities = [fitness_val / total_fitness for fitness_val in fitness_values]\n",
    "    cummulative_probabilities = np.cumsum(individual_probabilities)\n",
    "    if not elitism:\n",
    "        # Generate probabilities for new population.\n",
    "        r = np.random.rand(POP_SIZE)\n",
    "        # Get insertion points through a left bisect algorithm.\n",
    "        selected = np.searchsorted(cummulative_probabilities, r)\n",
    "        for idx in selected:\n",
    "            new_population.append(population[idx])\n",
    "    else:\n",
    "        best_fitness_values = sorted(fitness_values, reverse=True)[:ELITISM_NR]\n",
    "        chosen_elitism_values = [np.where(fitness_values == i)[0][0] for i in best_fitness_values]\n",
    "        # Generate probabilities for new population.\n",
    "        r = np.random.rand(POP_SIZE - ELITISM_NR)\n",
    "        # Get insertion points through a left bisect algorithm.\n",
    "        selected = np.searchsorted(cummulative_probabilities, r)\n",
    "        for idx in selected:\n",
    "            new_population.append(population[idx])\n",
    "        for idx in chosen_elitism_values:\n",
    "            new_population.append(population[idx])\n",
    "    return new_population\n",
    "\n",
    "\n",
    "def get_best_individual(population, fitness_values):\n",
    "    local_best = np.argmax(fitness_values)\n",
    "    best = fitness_values[local_best]\n",
    "    best_individual = population[local_best]\n",
    "    return best, best_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_smart_population(x_train, y_train, load=False):\n",
    "    if not load:\n",
    "        input_layer = Input(shape=(784,))\n",
    "        dense_1 = Dense(100, activation='sigmoid')(input_layer)\n",
    "        dense_2 = Dense(10, activation='sigmoid')(dense_1)\n",
    "        pred = Dense(10, activation='softmax')(dense_2)\n",
    "        model = Model(inputs=input_layer, outputs=pred)\n",
    "        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
    "        model.summary()\n",
    "        model.fit(x_train, to_categorical(y_train, num_classes=10), batch_size=256, epochs=1)\n",
    "        model.save('model.h5')\n",
    "        loss, acc = model.evaluate(x_train, to_categorical(y_train))\n",
    "    else:\n",
    "        model = load_model('model.h5')\n",
    "        loss, acc = model.evaluate(x_train, to_categorical(y_train))\n",
    "    print(f'Accuracy from the initial model: {acc}')\n",
    "\n",
    "    first_layer_weights = model.layers[1].get_weights()[0]\n",
    "    first_layer_biases = model.layers[1].get_weights()[1]\n",
    "    second_layer_weights = model.layers[2].get_weights()[0]\n",
    "    second_layer_biases = model.layers[2].get_weights()[1]\n",
    "    third_layer_weights = model.layers[3].get_weights()[0]\n",
    "    third_layer_biases = model.layers[3].get_weights()[1]\n",
    "\n",
    "    return [[np.copy(first_layer_weights), np.copy(second_layer_weights), np.copy(third_layer_weights),\n",
    "             np.copy(first_layer_biases), np.copy(second_layer_biases), np.copy(third_layer_biases)]\n",
    "            for _ in range(POP_SIZE)]\n",
    "\n",
    "\n",
    "def generate_population(units=N_UNITS):\n",
    "    return [[np.random.uniform(low=LOWER_BOUND,\n",
    "                               high=HIGHER_BOUND,\n",
    "                               size=(units[i], units[i+1])).astype('f')\n",
    "            for i in range(len(units) - 1)]\n",
    "            +\n",
    "            [np.random.uniform(low=LOWER_BOUND,\n",
    "                               high=HIGHER_BOUND,\n",
    "                               size=(units[i+1],)).astype('f')\n",
    "            for i in range(len(units) - 1)]\n",
    "           for _ in range(POP_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(use_back_prop=True, load=True):\n",
    "    start_time = time.time()\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        train_set, _, test_set = pickle.load(f, encoding='latin1')\n",
    "        x_train, y_train = train_set\n",
    "        x_test, y_test = test_set\n",
    "    if load:\n",
    "        if os.path.exists('population.pkl'):\n",
    "            with open('population.pkl', 'rb') as f:\n",
    "                population = pickle.load(f)\n",
    "        else:\n",
    "            if not use_back_prop:\n",
    "                population = generate_population()\n",
    "            else:\n",
    "                population = generate_smart_population(x_train, y_train, load=True)\n",
    "    else:\n",
    "        if not use_back_prop:\n",
    "            population = generate_population()\n",
    "        else:\n",
    "            population = generate_smart_population(x_train, y_train, load=True)\n",
    "\n",
    "    fitness_values = fitness_network(population, x_train, y_train)\n",
    "    best, best_individual = get_best_individual(population, fitness_values)\n",
    "    for i in range(NR_EPOCHS):\n",
    "        if i % 10 == 0:\n",
    "            with open('population.pkl', 'wb') as f:\n",
    "                pickle.dump(population, f)\n",
    "        print(f'Current epoch: {i}')\n",
    "        population = selection(population, fitness_values, elitism=False)\n",
    "        population = upgrade(population, cross_percentages=[.40, .55, .05])\n",
    "        fitness_values = fitness_network(population, x_train, y_train)\n",
    "        new_best, new_best_individual = get_best_individual(population, fitness_values)\n",
    "        print('Current best:', best)\n",
    "        print('New best:', new_best)\n",
    "        if new_best > best:\n",
    "            best = new_best\n",
    "            best_individual = new_best_individual\n",
    "            best_score = test_network(best_individual, x_train, y_train)\n",
    "            print(f'The network achieved an accuracy of {best_score * 100} percent on training set!')\n",
    "    best_score = test_network(best_individual, x_test, y_test)\n",
    "    print(f'The network achieved an accuracy of {best_score * 100} percent on testing set!')\n",
    "    print(f'Time taken: {time.time() - start_time} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 0\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.06730678191633749\n",
      "Current epoch: 1\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.06621717138583273\n",
      "Current epoch: 2\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.06351179306579609\n",
      "Current epoch: 3\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.05859578786172433\n",
      "Current epoch: 4\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.06381510724321734\n",
      "Current epoch: 5\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.05760336254172088\n",
      "Current epoch: 6\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.0619662614359498\n",
      "Current epoch: 7\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.060222539917784416\n",
      "Current epoch: 8\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.05986178982788527\n",
      "Current epoch: 9\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.06071745266817565\n",
      "Current epoch: 10\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.05437136009383444\n",
      "Current epoch: 11\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.054991240775038266\n",
      "Current epoch: 12\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.05245234691875538\n",
      "Current epoch: 13\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.054910414991905104\n",
      "Current epoch: 14\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.05451890900322983\n",
      "Current epoch: 15\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.054429623952501026\n",
      "Current epoch: 16\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.05411453830047733\n",
      "Current epoch: 17\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.055746152745960675\n",
      "Current epoch: 18\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.05605582195083628\n",
      "Current epoch: 19\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.048465458808322634\n",
      "Current epoch: 20\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04585939397997645\n",
      "Current epoch: 21\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04635781842891171\n",
      "Current epoch: 22\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.0507252022850705\n",
      "Current epoch: 23\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04743427426095114\n",
      "Current epoch: 24\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04586770865171051\n",
      "Current epoch: 25\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.051720502696321516\n",
      "Current epoch: 26\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04496887380764114\n",
      "Current epoch: 27\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.0409353522439757\n",
      "Current epoch: 28\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.043117600214506405\n",
      "Current epoch: 29\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04279198932419991\n",
      "Current epoch: 30\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.043500226095176835\n",
      "Current epoch: 31\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04550888083325424\n",
      "Current epoch: 32\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.041098884828013736\n",
      "Current epoch: 33\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.040918269275101636\n",
      "Current epoch: 34\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.0418774808820016\n",
      "Current epoch: 35\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04263070525892615\n",
      "Current epoch: 36\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.03980154217562127\n",
      "Current epoch: 37\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04154885822612766\n",
      "Current epoch: 38\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.038062274734984046\n",
      "Current epoch: 39\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.037663564687687986\n",
      "Current epoch: 40\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.03441999123764487\n",
      "Current epoch: 41\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.03923166775601092\n",
      "Current epoch: 42\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.045095460035977364\n",
      "Current epoch: 43\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.03804367139260377\n",
      "Current epoch: 44\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04092864548937495\n",
      "Current epoch: 45\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.038191402281087125\n",
      "Current epoch: 46\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.040103777703994974\n",
      "Current epoch: 47\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.04119493514618313\n",
      "Current epoch: 48\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.042034920640197766\n",
      "Current epoch: 49\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.040100393476535115\n",
      "Current epoch: 50\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.03888898837779911\n",
      "Current epoch: 51\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.035248939273313844\n",
      "Current epoch: 52\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.030505967473546676\n",
      "Current epoch: 53\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.03667358314323541\n",
      "Current epoch: 54\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.027294287586918597\n",
      "Current epoch: 55\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.03125939332066231\n",
      "Current epoch: 56\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.030661476654779633\n",
      "Current epoch: 57\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.029499859192134895\n",
      "Current epoch: 58\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.029885708539171787\n",
      "Current epoch: 59\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.026810882414980797\n",
      "Current epoch: 60\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.028739765146088364\n",
      "Current epoch: 61\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.02709675597501456\n",
      "Current epoch: 62\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.02689061273288877\n",
      "Current epoch: 63\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.027365302501739748\n",
      "Current epoch: 64\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.028963869544052858\n",
      "Current epoch: 65\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.029363581723502626\n",
      "Current epoch: 66\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.0286626509491735\n",
      "Current epoch: 67\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.030345863855547557\n",
      "Current epoch: 68\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.031240800155930244\n",
      "Current epoch: 69\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.026748515643416317\n",
      "Current epoch: 70\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.02738746407686406\n",
      "Current epoch: 71\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.027641094732249783\n",
      "Current epoch: 72\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.02940405096684324\n",
      "Current epoch: 73\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.027606972937687616\n",
      "Current epoch: 74\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.025661525480498364\n",
      "Current epoch: 75\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.027314499911871695\n",
      "Current epoch: 76\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.024183694199627175\n",
      "Current epoch: 77\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.021705944137134166\n",
      "Current epoch: 78\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.02311508487651784\n",
      "Current epoch: 79\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.024890671012522576\n",
      "Current epoch: 80\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.027366273664390553\n",
      "Current epoch: 81\n",
      "Current best: 0.07205270162192094\n",
      "New best: 0.02512936362701793\n",
      "Current epoch: 82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-7dd13e0c392e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_back_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-018fcb74e03f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(use_back_prop, load)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitness_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melitism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupgrade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_percentages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.55\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mfitness_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitness_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mnew_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_best_individual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_individual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitness_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current best:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-d9725dba1de6>\u001b[0m in \u001b[0;36mfitness_network\u001b[0;34m(population, x, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_weights\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0;31m# expit may be better, although it's debatable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(use_back_prop=False, load=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
